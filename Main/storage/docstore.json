{"docstore/metadata": {"2d0e8441-e013-4bf3-83b4-a080ccb6d96c": {"doc_hash": "bee0eb626e431a01a3e86cbf09e56dd0cefc42d842cfb768daabb84a49648d5e"}, "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4": {"doc_hash": "ad6732040d2454251242b90bd526ae0f663557f3c589e9a4324ebeccfc44b964"}, "8862f6ad-f95a-4f8a-a9a9-05989b95ffa7": {"doc_hash": "f094d6b4613427cb76f4d73610556b3e6ce27315db05df3053d1b99b0af9e2c7"}, "94575f14-7711-474c-8dc8-4f5f6ff72a82": {"doc_hash": "b6316452af9ccc7aa04c9a602fc91a9be1c775008b013842af76fba13de0fbff"}, "87522250-b3b0-4ca6-bd4c-26e984e24f59": {"doc_hash": "0652e4c994bc43ffe9f05d74488ca12cebda5ffc5b5e6008da86f97d299c31fb"}, "74027b37-dcc4-4ad8-b1a0-5c3d3987cacd": {"doc_hash": "c1c6eefbf7ac65193513dc6b1e970c953410e0546cd7c579cee1c7e646b341eb"}, "0a11270c-42d8-4352-acdb-0289ded4cbf2": {"doc_hash": "c113784cca63ccc94a3f82665f2876766cdeb5ba44e19a45cbf401b063411c26"}, "56b5ecd5-0897-49df-ae98-02ba53821a79": {"doc_hash": "c98557cc0495b02d6a28f424a7a36b2d609c3382010d5ef04ff005bcae14d065"}, "7c61277e-f05c-4a35-ac72-b22b46efa6cb": {"doc_hash": "94d110e6d92f31691227abcca0d01a8df9e783daeae083e01ae7bbea290dec07"}, "3759b0c6-b6cf-4ec9-86fc-38c15ca3bd34": {"doc_hash": "790e6ebddbb06e1fb209873b5b563026661a1af2ca7ba6873a2b88e363fc6851"}, "d196a493-9b3d-495d-a7a4-286a43981d93": {"doc_hash": "d67f04699ec9e2783394c00db263308ac4090ea0c4a5a42f154584b1ae43148f"}, "c430f613-40b9-478d-9c88-9db38a9ebe26": {"doc_hash": "cdd9e1c43757a1b743c3e52aa64753928d7904f45657a1615c4064d2fa2c79f2"}, "b7d1dc42-eaae-4bda-bc97-0316f6ade11d": {"doc_hash": "d155280f28424dfb37bde4e38967225073c09ca6e6203424931b27cd75239937"}, "311a0137-d750-4212-95fd-c30012c0de18": {"doc_hash": "efdfb1026a2f53353171a6f14b5090efb807be6c59a274a69c701b92ce16b7f4"}, "3625476a-13ad-4c5f-b0a8-851eb769b72a": {"doc_hash": "52e2da491170b931af258dea58432f67799c8ed4dda95e44f49bf5539c39bad3"}, "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f": {"doc_hash": "b7bec4febcd88896d16c543908751f76efcfe991ace911165a4f49f7d53f4ce7"}, "032e978a-bad2-4e71-a14d-94f679b6a8f1": {"doc_hash": "6572b8f8abc15d972ac4c3b1432232e9cf990b36703527d131fd4458bdfe0ae9"}, "9bc4559b-607c-466a-924c-954b567ca7fd": {"doc_hash": "a6fa3cbac1b7ddcab4b7d2d68dabfd531305793f620d77ec2678159365eb58c7", "ref_doc_id": "2d0e8441-e013-4bf3-83b4-a080ccb6d96c"}, "c8575643-4778-4965-98ae-a5594a807341": {"doc_hash": "df097c66ffc7f07a0850f9bf3ca50709fbdcc3e40d25c89127977db214e4abab", "ref_doc_id": "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4"}, "b24a178a-1987-4538-a8c4-3cc12226bd5c": {"doc_hash": "22db2cf670d465a34247774f4f606a2aa29483e43ba5c86c61700d5a689c772e", "ref_doc_id": "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4"}, "f6a99b19-37e9-48b8-b3c6-51dec37a5da4": {"doc_hash": "4da357e0909c620e6b8d2e238e58102ed7d216a4bedcdb9bf0aa8ab4c61e683a", "ref_doc_id": "8862f6ad-f95a-4f8a-a9a9-05989b95ffa7"}, "dc7e49d9-573a-4d5b-a6fe-97412a8fc217": {"doc_hash": "ae50e8173a2e4b19f81b0355017d165bda472b94c02de5df2edbe0e4ea771289", "ref_doc_id": "94575f14-7711-474c-8dc8-4f5f6ff72a82"}, "5da60b5c-834b-471c-a731-84161411871b": {"doc_hash": "16d0049a4c6a151e6a7d99e63e850315fab62d18f87403bb5d1b434e7c236b4b", "ref_doc_id": "87522250-b3b0-4ca6-bd4c-26e984e24f59"}, "680ade65-3ad7-4e80-91b8-0087f858d5be": {"doc_hash": "b173c6668dcfff8aaa9a362d33cd923b02c1e03bab1c36aadd00ab7cea8c6568", "ref_doc_id": "74027b37-dcc4-4ad8-b1a0-5c3d3987cacd"}, "eac8339d-345a-45af-8a84-aeef738904db": {"doc_hash": "b21dfc4c929e597e883dd4ba41b7ffbd5865efcbdf7d437ef97e9d9c2386a102", "ref_doc_id": "0a11270c-42d8-4352-acdb-0289ded4cbf2"}, "17cf43bb-a7a0-4f6c-b9ee-4f19fd8fcb38": {"doc_hash": "59ac4414a21db24ae2ab7a5d4e871b5810000646e8b05689b7566f412e3f951b", "ref_doc_id": "56b5ecd5-0897-49df-ae98-02ba53821a79"}, "95f9264f-1223-4231-b66f-e111ea111551": {"doc_hash": "faa0fc3fa2b9230ecc94b1a9d35cbee12c0097880861535aff4990f4c248c03d", "ref_doc_id": "56b5ecd5-0897-49df-ae98-02ba53821a79"}, "11d7f0ea-9ce5-4e9e-b816-212c6f637cff": {"doc_hash": "3d07449a5586c8cfe2fe7d48c0a7420fd8a59968b2100721e5e14ca00b77da09", "ref_doc_id": "7c61277e-f05c-4a35-ac72-b22b46efa6cb"}, "b6a7040a-3483-4861-97ee-a3fc9febeff9": {"doc_hash": "b84db1f5a2ed65085f406b0888bdd1b1722c0dc1224cbc735df7295c7958d583", "ref_doc_id": "7c61277e-f05c-4a35-ac72-b22b46efa6cb"}, "af88c077-f8b9-4f08-8c0b-5a4f8acb0034": {"doc_hash": "7f58d5ef97f7ad8af11a77bcd00a1148d4042a06d9ba1064d4f11eaca0977348", "ref_doc_id": "3759b0c6-b6cf-4ec9-86fc-38c15ca3bd34"}, "008ca87d-10f8-425d-84ca-769bdb2e1fe5": {"doc_hash": "b25f15b077bdc6195cbeb63c11996f7927081b953fb0b4aebdb12dc88da6813f", "ref_doc_id": "d196a493-9b3d-495d-a7a4-286a43981d93"}, "3e99a89a-1972-4655-ac8b-f0f0a3c60c62": {"doc_hash": "a1b556f36856380bdafa670af8eeae887e6ccb830e3a565ca6ecddfc01431b11", "ref_doc_id": "c430f613-40b9-478d-9c88-9db38a9ebe26"}, "7548b177-bd98-4b89-abec-255c876b6e03": {"doc_hash": "b8e38bc633fce74e7cdc2fd7f75f8dc1b39cbe1b65ddec50ee7477a7035569c3", "ref_doc_id": "b7d1dc42-eaae-4bda-bc97-0316f6ade11d"}, "8b576b3a-a919-47ed-b6e6-a1e30dc8ea2c": {"doc_hash": "9a0aa5d858d4e58a1195327fc7c9dc9fea171e7eed2aae0edc9bd4f6b9de8b31", "ref_doc_id": "311a0137-d750-4212-95fd-c30012c0de18"}, "d411ca0b-03db-4cb4-bf84-3d400df92f37": {"doc_hash": "0ec95cbdf68a89e116b29d95d6a4dcb67e9f442e542a58cae071874fc66e6629", "ref_doc_id": "3625476a-13ad-4c5f-b0a8-851eb769b72a"}, "55dfef50-accd-4878-9f0c-8bd1720f9035": {"doc_hash": "796c16e5185d82ec07ad325ecbb9efaad62c771722df6773e2b83f35384fc0cb", "ref_doc_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f"}, "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d": {"doc_hash": "e2133185bc5d20f4684e8a5059845e9739b4fe35d6516cc79a424ce10595c19a", "ref_doc_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f"}, "a53677d0-13bb-4a05-94e0-20cbba1a835d": {"doc_hash": "8699fe6712353e06432b621d15eb0815644fba8ebc4fe478f83b00eb800c3895", "ref_doc_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f"}, "13a8ff70-6ce4-46b6-b437-e41a0a039f9a": {"doc_hash": "7cabf7084c379cda199400f1a110a9d07917515ff1b11414c0f85b8e36aaa97a", "ref_doc_id": "032e978a-bad2-4e71-a14d-94f679b6a8f1"}}, "docstore/ref_doc_info": {"2d0e8441-e013-4bf3-83b4-a080ccb6d96c": {"node_ids": ["9bc4559b-607c-466a-924c-954b567ca7fd"], "metadata": {"page_label": "1", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4": {"node_ids": ["c8575643-4778-4965-98ae-a5594a807341", "b24a178a-1987-4538-a8c4-3cc12226bd5c"], "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "8862f6ad-f95a-4f8a-a9a9-05989b95ffa7": {"node_ids": ["f6a99b19-37e9-48b8-b3c6-51dec37a5da4"], "metadata": {"page_label": "3", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "94575f14-7711-474c-8dc8-4f5f6ff72a82": {"node_ids": ["dc7e49d9-573a-4d5b-a6fe-97412a8fc217"], "metadata": {"page_label": "4", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "87522250-b3b0-4ca6-bd4c-26e984e24f59": {"node_ids": ["5da60b5c-834b-471c-a731-84161411871b"], "metadata": {"page_label": "5", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "74027b37-dcc4-4ad8-b1a0-5c3d3987cacd": {"node_ids": ["680ade65-3ad7-4e80-91b8-0087f858d5be"], "metadata": {"page_label": "6", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "0a11270c-42d8-4352-acdb-0289ded4cbf2": {"node_ids": ["eac8339d-345a-45af-8a84-aeef738904db"], "metadata": {"page_label": "7", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "56b5ecd5-0897-49df-ae98-02ba53821a79": {"node_ids": ["17cf43bb-a7a0-4f6c-b9ee-4f19fd8fcb38", "95f9264f-1223-4231-b66f-e111ea111551"], "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "7c61277e-f05c-4a35-ac72-b22b46efa6cb": {"node_ids": ["11d7f0ea-9ce5-4e9e-b816-212c6f637cff", "b6a7040a-3483-4861-97ee-a3fc9febeff9"], "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "3759b0c6-b6cf-4ec9-86fc-38c15ca3bd34": {"node_ids": ["af88c077-f8b9-4f08-8c0b-5a4f8acb0034"], "metadata": {"page_label": "10", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "d196a493-9b3d-495d-a7a4-286a43981d93": {"node_ids": ["008ca87d-10f8-425d-84ca-769bdb2e1fe5"], "metadata": {"page_label": "11", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "c430f613-40b9-478d-9c88-9db38a9ebe26": {"node_ids": ["3e99a89a-1972-4655-ac8b-f0f0a3c60c62"], "metadata": {"page_label": "12", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "b7d1dc42-eaae-4bda-bc97-0316f6ade11d": {"node_ids": ["7548b177-bd98-4b89-abec-255c876b6e03"], "metadata": {"page_label": "13", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "311a0137-d750-4212-95fd-c30012c0de18": {"node_ids": ["8b576b3a-a919-47ed-b6e6-a1e30dc8ea2c"], "metadata": {"page_label": "14", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "3625476a-13ad-4c5f-b0a8-851eb769b72a": {"node_ids": ["d411ca0b-03db-4cb4-bf84-3d400df92f37"], "metadata": {"page_label": "15", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f": {"node_ids": ["55dfef50-accd-4878-9f0c-8bd1720f9035", "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d", "a53677d0-13bb-4a05-94e0-20cbba1a835d"], "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}, "032e978a-bad2-4e71-a14d-94f679b6a8f1": {"node_ids": ["13a8ff70-6ce4-46b6-b437-e41a0a039f9a"], "metadata": {"page_label": "17", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}}}, "docstore/data": {"9bc4559b-607c-466a-924c-954b567ca7fd": {"__data__": {"id_": "9bc4559b-607c-466a-924c-954b567ca7fd", "embedding": null, "metadata": {"page_label": "1", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d0e8441-e013-4bf3-83b4-a080ccb6d96c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "bee0eb626e431a01a3e86cbf09e56dd0cefc42d842cfb768daabb84a49648d5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A scalable machine learning \nstrategy for resource allocation in \ndatabase\nFady Nashat Manhary2, Marghny H. Mohamed1\uf02a & Mamdouh Farouk2\nModern cloud computing systems require intelligent resource allocation strategies that balance \nquality-of-service (QoS), operational costs, and energy sustainability. Existing deep Q-learning (DQN) \nmethods suffer from sample inefficiency, centralization bottlenecks, and reactive decision-making \nduring workload spikes. Transformer-based forecasting models such as Temporal Fusion Transformer \n(TFT) offer improved accuracy but introduce computational overhead, limiting real-time deployment. \nWe propose LSTM-MARL-Ape-X, a novel framework integrating bidirectional Long Short-Term \nMemory (BiLSTM) for workload forecasting with Multi-Agent Reinforcement Learning (MARL) in a \ndistributed Ape-X architecture. This approach enables proactive, decentralized, and scalable resource \nmanagement through three innovations: high-accuracy forecasting using BiLSTM with feature-wise \nattention, variance-regularized credit assignment for stable multi-agent coordination, and faster \nconvergence via adaptive prioritized replay. Experimental validation on real-world traces demonstrates \n94.6% SLA compliance, 22% reduction in energy consumption, and linear scalability to over 5,000 \nnodes with sub-100 ms decision latency. The framework converges 3.2\u00d7 faster than uniform sampling \nbaselines and outperforms transformer-based models in both accuracy and inference speed. Unlike \ndecoupled prediction-action frameworks, our method provides end-to-end optimization, enabling \nrobust and sustainable cloud orchestration at scale.\nKeywords Cloud computing, Resource allocation, Multi-Agent Reinforcement Learning, Workload \nforecasting, Energy efficiency, Scalability\nModern cloud computing systems face escalating demands for resource efficiency , QoS guarantees , and \nsustainability. Reinforcement Learning (RL) has emerged as a promising approach to dynamic resource \nallocation. However, existing RL-based methods suffer from three key limitations1,2:\n\u2022 Sample inefficiency: (DQN) methods require millions of time steps to converge in realistic cloud environ -\nments, rendering them impractical for real-time applications1.\n\u2022 Centralization bottlenecks : Centralized single-agent architectures experience instability when managing \nmore than 500 (VMs), with decision latency growing linearly and exceeding 200ms2.\n\u2022 Reactive behavior: Traditional RL techniques fail to anticipate workload trends, leading to a 26% increase in \nSLA violations during traffic spikes3.\nRecent research has proposed partial solutions to these limitations:\n\u2022 Transformer-based forecasting improves prediction accuracy but introduces substantial computational \noverhead, with inference latencies often surpassing 50ms4.\n\u2022 Distributed RL frameworks improve scalability but often lack coordination strategies suitable for resource \nmanagement in cloud infrastructures5.\n\u2022 Hybrid prediction-RL models combine forecasting with decision-making but remain loosely coupled, pre -\nventing end-to-end optimization6.\nTo address these challenges holistically, we introduce LSTM-MARL-Ape-X, a unified framework that delivers \nthree major innovations: \n1Computer and Information Technology, Egypt-Japan University of Science and Technology (E-JUST), New Borg \nEl-Arab City 21934, Alexandria, Egypt. 2Department of Computer Science, Faculty of Computers and Information, \nAssiut University, Assiut, Egypt. \uf02aemail: marghny.mohamed@ejust.edu.eg\nOPEN\nScientific Reports |        (2025) 15:30567 1| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8575643-4778-4965-98ae-a5594a807341": {"__data__": {"id_": "c8575643-4778-4965-98ae-a5594a807341", "embedding": null, "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "ad6732040d2454251242b90bd526ae0f663557f3c589e9a4324ebeccfc44b964", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b24a178a-1987-4538-a8c4-3cc12226bd5c", "node_type": "1", "metadata": {}, "hash": "fdc4d44ea398c41f33470a45da283d25a2f6429774211eba9e6bcbadefe9ee7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. Proactive decision-making: A BiLSTM model with feature-wise attention achieves 94.56% prediction accu-\nracy while maintaining low inference latency (2.7ms).\n 2. Decentralized coordination: (MARL) framework with variance-regularized credit assignment reduces SLA \nviolations by 72% compared to traditional single-agent DQN methods.\n 3. Sample-efficient training: An improved Ape-X architecture incorporating adaptive prioritized experience \nreplay converges 3.2\u00d7 faster than models using uniform sampling.\nOur key contributions are summarized as follows:\n\u2022 We propose the first unified framework  that integrates LSTM-based workload forecasting with MARL for \ndynamic cloud resource allocation, achieving 6.5% higher SLA compliance than (TFT)4.\n\u2022 We introduce a novel credit assignment mechanism that stabilizes multi-agent learning and enables linear \nscalability to over 5,000 cloud nodes.\n\u2022 We validate our approach using real-world production traces from Microsoft Azure 7 and Google Cloud 3, \ndemonstrating a 22% reduction in energy consumption through carbon-aware (VM) placement.\nThe remainder of this paper is organized as follows:\n\u2022 Section Related work presents the related work relevant to cloud resource management, forecasting models, \nand reinforcement learning techniques.\n\u2022 Section\u00a0Results provides the experimental results and performance evaluation of the proposed framework.\n\u2022 Section\u00a0Discussion discusses the key findings, implications, and limitations of the results.\n\u2022 Section\u00a0 Methods describes the methods used, including the system architecture, training procedure, and \nbaseline configurations.\nRelated work\nCloud resource allocation has evolved through three major paradigms: (1) rule-based heuristics, (2) machine \nlearning-driven optimization, and (3) integrated learning systems. Below, we analyze each paradigm and \nhighlight critical gaps that our work addresses.\nWorkload forecasting techniques\nEarly statistical models such as Autoregressive Integrated Moving Average ARIMA 8 achieved moderate \nprediction accuracy (60-75%) for cloud workloads but struggled with non-stationary and bursty traffic patterns9. \nMore recent approaches leveraging LSTM networks 10 improved accuracy to 85-90% by capturing long-range \ntemporal dependencies. However, these models have two main drawbacks: (1) unidirectional processing causes \ndelayed detection of abrupt workload changes, incurring latencies around 200ms3, and (2) decoupled forecasting \narchitectures propagate prediction errors to downstream resource managers, limiting overall performance.\nTransformer-based models such as the Temporal Fusion Transformer (TFT)4 introduced multi-head attention \nfor multivariate time series forecasting, achieving 91.2% accuracy on Microsoft Azure traces. Nonetheless, TFT\u2019s \nquadratic complexity O(n2) in sequence length renders it computationally expensive for real-time deployment, \nwith experiments showing 3.1 \u00d7 higher Graphics Processing Unit (GPU) memory usage compared to LSTM-\nbased methods11.\nReinforcement learning in cloud management\nDeep RL approaches like (DQN) demonstrated promising VM consolidation results, reducing energy \nconsumption by 15-20% in small clusters 1. However, DQN\u2019s centralized design scales poorly, with decision \nlatency growing linearly (R2 = 0.97) and instability appearing beyond 500 nodes2. Techniques such as Prioritized \nExperience Replay12 enhance sample efficiency but introduce bias towards rare states, problematic for diurnal \ncloud workloads13.\nDistributed RL frameworks like Ape-X 14 leverage parallel actor learners to improve scalability but lack \ncoordination mechanisms for managing interdependent cloud resources (CPU, GPU, network) and fail to \nintegrate predictive models for demand anticipation. Analysis of IMPALA 15 on Google Cloud traces revealed \n18-26% more SLA violations during auto-scaling events compared to oracle provisioning16.\nHybrid prediction-action systems\nHybrid frameworks coupling workload forecasting with RL policies attempt to bridge prediction and control \nbut often suffer from cascading errors. For example, 6 employ a two-stage pipeline (LSTM prediction followed \nby DQN control) incurring an additional 43ms latency relative to end-to-end models. Similarly, 5 apply MARL \nfor container orchestration but rely on simple average credit assignment, leading to 37% higher reward variance \ncompared to our proposed variance-regularized credit assignment.\nMulti-agent coordination\n(MARL) in cloud settings confronts unique challenges: (1) partial observability of distributed resource states, \n(2) delayed and sparse rewards complicating credit assignment, and (3) non-stationary dynamics due to \ncompeting agents.", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 4710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b24a178a-1987-4538-a8c4-3cc12226bd5c": {"__data__": {"id_": "b24a178a-1987-4538-a8c4-3cc12226bd5c", "embedding": null, "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "13bc5a50-780a-4e9d-be99-c1fd3b7e96d4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "ad6732040d2454251242b90bd526ae0f663557f3c589e9a4324ebeccfc44b964", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8575643-4778-4965-98ae-a5594a807341", "node_type": "1", "metadata": {"page_label": "2", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "df097c66ffc7f07a0850f9bf3ca50709fbdcc3e40d25c89127977db214e4abab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Analysis of IMPALA 15 on Google Cloud traces revealed \n18-26% more SLA violations during auto-scaling events compared to oracle provisioning16.\nHybrid prediction-action systems\nHybrid frameworks coupling workload forecasting with RL policies attempt to bridge prediction and control \nbut often suffer from cascading errors. For example, 6 employ a two-stage pipeline (LSTM prediction followed \nby DQN control) incurring an additional 43ms latency relative to end-to-end models. Similarly, 5 apply MARL \nfor container orchestration but rely on simple average credit assignment, leading to 37% higher reward variance \ncompared to our proposed variance-regularized credit assignment.\nMulti-agent coordination\n(MARL) in cloud settings confronts unique challenges: (1) partial observability of distributed resource states, \n(2) delayed and sparse rewards complicating credit assignment, and (3) non-stationary dynamics due to \ncompeting agents. The COMA algorithm17 uses counterfactual baselines but suffers from scalability bottlenecks \nin centralized critics when scaling beyond 1,000 VMs 7. Decentralized approaches such as MADDPG 18 avoid \nthis bottleneck but show 29% higher SLA violations than centralized methods in our Azure environment tests19.\nScientific Reports |        (2025) 15:30567 2| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 3771, "end_char_idx": 5143, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f6a99b19-37e9-48b8-b3c6-51dec37a5da4": {"__data__": {"id_": "f6a99b19-37e9-48b8-b3c6-51dec37a5da4", "embedding": null, "metadata": {"page_label": "3", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8862f6ad-f95a-4f8a-a9a9-05989b95ffa7", "node_type": "4", "metadata": {"page_label": "3", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "f094d6b4613427cb76f4d73610556b3e6ce27315db05df3053d1b99b0af9e2c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Innovation positioning\nOur approach introduces a novel, integrated framework for carbon-aware auto-scaling in cloud environments, \nstanding out in several respects:\n\u2022 Multi-Objective Optimization: Unlike conventional auto-scaling focusing on single metrics (e.g., perfor -\nmance or cost), we optimize across performance, energy use, carbon footprint, and financial cost to enable \nsustainable cloud operations20.\n\u2022 Carbon-Aware Intelligence: We incorporate real-time carbon intensity signals into the decision-making \nloop via masking and reward shaping, allowing preference for low-carbon scheduling where feasible21.\n\u2022 Temporal Forecasting Fusion: We combine BiLSTMs with attention mechanisms and RL to anticipate work-\nload fluctuations proactively22.\n\u2022 End-to-End Learning Architecture:  Our design integrates prioritized experience replay and novel credit \nassignment mechanisms to enable robust, efficient training in dynamic cloud environments23.\n\u2022 Training Optimization: We apply adaptive learning rate schedules, early stopping, and replay buffer prioriti-\nzation, enhancing convergence speed and generalization to unseen workloads24.\n\u2022 Decentralized Coordination: Our architecture supports decentralized policy execution among agents with \nshared situational awareness, preserving autonomy while enabling collaboration\u2014essential for large-scale, \npartially observable cloud systems25.\nThese contributions position our framework at the nexus of cloud computing, AI, and sustainability, offering a \npractical and scalable solution for green cloud auto-scaling.\nResults\nThese training dynamics confirm the efficacy of our integrated design, where accurate forecasting supports \nscalable, efficient decision-making through decentralized RL. Figure\u00a0 1 illustrates the training evolution across \nkey model components and system performance indicators.\nWorkload prediction performance\nOur BiLSTM forecaster\u2019s performance was rigorously evaluated against five state-of-the-art baselines using \nproduction traces from Google Cloud and Microsoft Azure. The combined evidence from Table\u00a0 1 and Fig.\u00a0 2 \ndemonstrates that our approach achieves superior accuracy while maintaining real-time operational efficiency.\nKey Findings\n 1. Accuracy Improvements:\n\u2022 31.6% lower MAE than TFT (4.89 vs. 7.15) with 19\u00d7 faster inference\n\u2022 16.8% improvement over Mamba while maintaining linear scalability\n\u2022 R2 score of 0.95 indicates excellent fit to workload patterns\n \n 2. Architectural Advantages (evident in Fig.\u00a02):\n\u2022 Feature Selection: Network metrics receive 62% higher attention than disk I/O\n\u2022 Temporal Adaptation: Hour-of-day attention correlates with actual traffic (r =0 .82)\n\u2022 Burst Handling: Disk write attention spikes precede load increases by 3 timesteps\n \n 3. Performance Drivers:\n\u2022 Bidirectional Processing: 200ms faster spike detection than unidirectional LSTM (p<0.01)\n\u2022 Attention Mechanism: 18% error reduction versus uniform feature weighting\n\u2022 Quantile Outputs: 90% prediction intervals 23% narrower than TFTComparative Analysis\nThe attention patterns in Fig.\u00a02 explain why alternatives underperform:\n\u2022 TFT: Quadratic complexity from dense attention across all features\n\u2022 Mamba: Sequential processing misses backward dependencies\n\u2022 MAPPO: Centralized coordination increases latency (Table\u00a01)Reproducibility\nAll results were obtained under controlled conditions:\n\u2022 Datasets: Google Cluster (12k nodes) and Azure VM traces\n\u2022 Splits: 70/15/15 train/validation/test (stratified)\n\u2022 Hardware: NVIDIA V100 GPUs (32GB memory)\n\u2022 Statistics: 5 random seeds (95% CI \u22641.8%)\nThe combination of quantitative results (Table\u00a01) and qualitative insights (Fig.\u00a02) demonstrates that our BiLSTM \nforecaster achieves state-of-the-art performance through intelligent feature prioritization and efficient temporal \nprocessing.\nScientific Reports |        (2025) 15:30567 3| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "dc7e49d9-573a-4d5b-a6fe-97412a8fc217": {"__data__": {"id_": "dc7e49d9-573a-4d5b-a6fe-97412a8fc217", "embedding": null, "metadata": {"page_label": "4", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "94575f14-7711-474c-8dc8-4f5f6ff72a82", "node_type": "4", "metadata": {"page_label": "4", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "b6316452af9ccc7aa04c9a602fc91a9be1c775008b013842af76fba13de0fbff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "QoS and scalability metrics\nTo assess the scalability and Quality of Service (QoS) performance of the proposed LSTM-MARL-Ape-X \nframework, we conducted a comprehensive set of stress tests on a 5,000-node cloud environment. Our framework \nwas benchmarked against several state-of-the-art baselines, including traditional threshold-based autoscaling \n(TAS), deep Q-networks (DQN), transformer-based reinforcement learning (TFT+RL), MAPPO, and \nMamba+RL. Table\u00a02 presents a comparative summary of key performance metrics, including SLA compliance, \nviolation rates, energy consumption, end-to-end latency, and scalability behavior.\nMethod MAE RMSE R2 Inference Latency (ms)\nARIMA 12.34 15.67 0.68 1.2\nLSTM 8.21 10.45 0.85 2.1\nTFT 7.15 9.32 0.91 51.3\nMamba 6.02 8.15 0.93 4.2\nMAPPO 5.87 7.95 0.94 7.2\nOur BiLSTM 4.89 6.78 0.95 2.7\nTable 1. Workload prediction performance comparison (lower values are better for MAE/RMSE). Bold font \nhighlights the performance values achieved by the proposed algorithm.\n \nFig. 1. Training Dynamics of the LSTM-MARL-Ape-X Framework (a) BiLSTM Validation Pinball Loss: \nX-axis shows training epochs (0\u2013100). Y-axis shows pinball loss (lower is better). Lines represent 10% (blue), \n50% (green), and 90% (orange) quantiles. All quantiles decrease monotonically, with the 90% quantile showing \n62% error reduction by epoch 50. (b) MARL Reward and Variance: X-axis shows environment steps (0\u2013400k). \nLeft y-axis shows average reward (purple line, scale: \u221210 to +25). Right y-axis shows reward variance (yellow \nband, \u00b11\u03c3). Reward stabilizes at +22.4 with 78% variance reduction. (c) Policy Entropy: X-axis shows \nenvironment steps (0\u2013400k). Y-axis shows entropy in bits (scale: 0\u20134). Entropy drops from 3.8 to 0.6 bits, \nindicating policy convergence. (d) SLA Compliance vs. Scaling Efficiency: X-axis shows environment steps \n(0\u2013400k). Left y-axis shows SLA compliance (green line, 0\u2013100%). Right y-axis shows scaling efficiency (pink \nline, 0\u20131.0). The system achieves 94.6% SLA compliance with 0.35 scaling efficiency.\n \nScientific Reports |        (2025) 15:30567 4| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2172, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "5da60b5c-834b-471c-a731-84161411871b": {"__data__": {"id_": "5da60b5c-834b-471c-a731-84161411871b", "embedding": null, "metadata": {"page_label": "5", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87522250-b3b0-4ca6-bd4c-26e984e24f59", "node_type": "4", "metadata": {"page_label": "5", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "0652e4c994bc43ffe9f05d74488ca12cebda5ffc5b5e6008da86f97d299c31fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Quantitative Results:  As summarized in Table\u00a0 2, our LSTM-MARL-Ape-X framework demonstrated \nsuperior performance across all evaluation criteria:\n\u2022 Achieved 94.6% SLA compliance, representing a 6.4% improvement over MAPPO\n\u2022 Reduced violations to just 0.5/hour\u2014a 54.5% decrease compared to MAPPO\nMethod SLA Compliance (%) Violations (/hr)\nEnergy\n(kWh)\nEnd-to-End Latency\n(ms) Scalability\nThreshold Auto-Scaling (TAS) 82.1 3.2 412 12.4 Linear\nDQN 85.7 2.1 387 9.8 Sublinear\nTFT+RL 88.2 1.8 365 8.5 Logarithmic\nMAPPO 91.3 1.1 328 7.2 Linear\nMamba+RL 90.6 1.4 338 6.9 Linear\nLSTM-MARL-Ape-X 94.6 0.5 298 5.1 Linear\nTable 2. QoS and scalability performance at 5000 nodes. Bold font highlights the performance values achieved \nby the proposed algorithm.\n \nFig. 2. Learned attention weights in the BiLSTM workload forecaster. The heatmap shows normalized \nattention weights (0-0.25 scale) across (1) four resource metrics (rows: CPU, memory, disk I/O, and network) \nand (2) three temporal features (rows: hour-of-day, day-of-week, and minute-of-hour) for six historical \ntimesteps (T-6 to T-0, columns). Key observations: (1) Network inbound traffic maintains sustained high \nattention (0.25 at T-0), (2) Disk write attention spikes precede load increases by 3 timesteps, and (3) Hour-of-\nday attention shows strong cyclical patterns (r =0 .82 with actual traffic). The selective attention to network \nand temporal features explains the model\u2019s 18% lower prediction error compared to uniform weighting \nbaselines.\n \nScientific Reports |        (2025) 15:30567 5| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1633, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "680ade65-3ad7-4e80-91b8-0087f858d5be": {"__data__": {"id_": "680ade65-3ad7-4e80-91b8-0087f858d5be", "embedding": null, "metadata": {"page_label": "6", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "74027b37-dcc4-4ad8-b1a0-5c3d3987cacd", "node_type": "4", "metadata": {"page_label": "6", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "c1c6eefbf7ac65193513dc6b1e970c953410e0546cd7c579cee1c7e646b341eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Consumed 298 kWh, yielding a 22% reduction in energy usage relative to TAS\n\u2022 Maintained linear scalability across 5,000 nodesLatency Analysis: The observed latency growth stems from \nthree key factors:\n\u2022 Coordination overhead: Centralized methods (DQN, TFT+RL) exhibited O(n2) message complexity, with \nTFT+RL \u2019s 112ms latency at scale attributed to its transformer\u2019s quadratic attention scaling\n\u2022 State synchronization: MAPPO\u2019s 7.2ms baseline latency included 3.1ms (43%) for parameter server syn -\nchronization\n\u2022 Monitoring burden: Conventional approaches allocated 35-48% of latency to metric collection, while our \ndistributed LSTM observers reduced this to 12% via edge-cached temporal embeddings Energy Efficiency \nBaselines:\n\u2022 TAS (412 kWh): Represents traditional autoscaling without RL optimization\n\u2022 MAPPO (328 kWh): Serves as our multi-agent RL baseline with centralized critic\n\u2022 Mamba+RL (338 kWh): Provides the SSM-based efficiency reference point\n\u2022 Improvements are measured against the best-performing baseline for each metric (MAPPO for SLA, TAS for \nenergy)Key Improvements:\n\u2022 Variance-regularized credit assignment reduced SLA violations by 72% versus DQN (0.5 vs 2.1/hr) through \n\u00b115% advantage normalization\n\u2022 Carbon-aware action masking decreased energy usage by 18.3% compared to Mamba+RL (298 vs 338 kWh) \nby constraining power-hungry actions during peak carbon periods\n\u2022 Distributed LSTM observers achieved 5.1ms latency (41% reduction vs TAS) via localized observation win-\ndowsDiscussion:\n\u2022 Architecture limits: DQN\u2019s sublinear scalability resulted from replay buffer congestion (78% CPU utilization \nat 3k nodes)\n\u2022 Energy tradeoffs: Mamba+RL \u2019s 90.6% SLA compliance came at 9.8% higher energy cost than our solution \ndue to unconstrained state space growth\n\u2022 Practical thresholds: MAPPO maintained viability up to \u223c3,200 nodes before experiencing 2\u00d7 latency deg-\nradationReproducibility:\n\u2022 Platform: Google Cloud (n1-standard-16 instances, Carbon-aware computing enabled)\n\u2022 Energy measurement: Cloud Monitoring API \u00b1/2% accuracy, normalized to 24h kWh at 80% utilization\n\u2022 Test Duration: 24-hour stress tests with diurnal workload patterns\n\u2022 Metrics: Averaged across 5 random seeds (95% confidence intervals \u22641.8%)\n\u2022 Baseline versions: TAS (Kubernetes VPA), DQN/TFT+RL (RLlib 2.0), MAPPO (PyMARL2), Mamba+RL \n(custom JAX impl.)\nDecision latency\nBaseline Comparison\nTable\u00a03 shows end-to-end decision latency across cluster sizes.\nQuantitative Results Our approach maintained sub-100ms latency at 5,000 nodes, achieving:\n\u2022 4.9\u00d7 faster than TFT+RL\n\u2022 3.6\u00d7 faster than MAPPO\n\u2022 2.7\u00d7 faster than Mamba\nKey Improvements\n\u2022 Distributed MARL architecture reduced coordination overhead (38% less than MAPPO)\n\u2022 Lightweight BiLSTM (2.7ms inference) enabled faster decisions vs Mamba\u2019s 4.2ms\n\u2022 Asynchronous policy updates prevented learner bottlenecks (12% faster than Mamba\u2019s windowed approach)\nMethod 500 Nodes 1,000 Nodes 2,000 Nodes 5,000 Nodes Scalability\nTAS 5 5 5 5 Fixed\nDQN 32 64 128 320 Linear\nTFT+RL 56 98 184 460 Quadratic\nMAPPO 42 75 142 355 Linear\nMamba 28 51 97 243 Sub-linear\nOurs 18 32 59 89 Sub-linear\nTable 3. Decision latency comparison (ms). Bold font highlights the performance values achieved by the \nproposed algorithm.\n \nScientific Reports |        (2025) 15:30567 6| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eac8339d-345a-45af-8a84-aeef738904db": {"__data__": {"id_": "eac8339d-345a-45af-8a84-aeef738904db", "embedding": null, "metadata": {"page_label": "7", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0a11270c-42d8-4352-acdb-0289ded4cbf2", "node_type": "4", "metadata": {"page_label": "7", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "c113784cca63ccc94a3f82665f2876766cdeb5ba44e19a45cbf401b063411c26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discussion While TAS had lowest latency (fixed 5ms), it lacked adaptability. Mamba showed promising sub-\nlinear scaling but required sequential processing. Our LSTM-MARL-Ape-X provides:\n\u2022 Near-TAS latency with intelligent decision-making\n\u2022 Better scalability than MAPPO\u2019s centralized critic\n\u2022 Lower variance than Mamba in large clusters (\u00b1=2.1ms vs 3.8ms at 5k nodes)\nReproducibility Details Latency measured from observation to completed action (10-run averages). Network \nlatency included (5ms RTT between nodes, \u00b10.8ms jitter). All tests used NVIDIA V100 GPUs with 32GB \nmemory.\nTraining convergence speed\nBaseline Comparison Table\u00a04 compares training efficiency metrics across six approaches.\nQuantitative Results LSTM-MARL-Ape-X achieved:\n\u2022 380k steps to converge (3.1\u00d7 faster than DQN)\n\u2022 0.89 sample efficiency (14% better than MAPPO)\n\u2022 24.6 final reward (2.9% higher than MAPPO)\n\u2022 38 GPU hours (15% less than MAPPO)Key Improvements\n\u2022 Adaptive prioritized replay: (\u03b1 =0 .6, \u03b2 =0 .5 \u2192 0.1) improved sample reuse by 27% versus Mamba\n\u2022 Forecast-aware prioritization: Focused training on critical transitions (18% reduction in wasted samples)\n\u2022 Decentralized learners : Enabled parallel gradient updates (1.9 \u00d7 speedup over MAPPO\u2019s centralized up -\ndates)\n\u2022 Carbon-aware scheduling: Reduced energy-intensive training steps by 22% versus baselinesDiscussion The \nenhanced Ape-X architecture provides:\n\u2022 Better stability than vanilla experience replay (38% lower reward variance)\n\u2022 Faster convergence than sequential models like Mamba (1.7\u00d7 speedup)\n\u2022 More efficient coordination than MAPPO (24% lower communication overhead)Reproducibility Details\n\u2022 Convergence criterion: \u2206reward < 0.1% for 10k steps\n\u2022 Hardware: Uniform NVIDIA V100 GPUs (32GB memory)\n\u2022 Workload: Microsoft Azure trace dataset\n\u2022 5 random seeds per method (95% CI \u2264 1.2%)\nAblation study\nComponent-Wise Impact Analysis\nTo understand the contribution of each architectural component, we conducted an ablation study by \nsystematically removing individual modules from the full model and measuring the resulting change in SLA \ncompliance. Table\u00a05 presents the observed performance drop and associated insights.\nQuantitative Insights\nComponent Removed Impact on SLA Compliance Key Observation\nBiLSTM \u2212\u00a05.4% Unidirectional processing delayed spike detection.\nAttention Mechanism \u2212\u00a03.2% Reduced focus on critical temporal features.\nVariance-Regularized Credit \u2212\u00a06.7% Increased reward instability among agents.\nPrioritized Replay \u2212\u00a04.5% Slower convergence (570k steps to converge).\nCarbon Masking \u2212\u00a02.3% Higher energy use (+15%) with marginal QoS gain.\nTable 5. Ablation Study: Component-wise Impact on SLA Compliance and Key Observations.\n \nMethod Steps to Converge GPU Hours Sample Efficiency Final Reward Speedup vs DQN\nDQN 1.2M 48 0.41 18.7 1.0\u00d7\nTFT+RL 950k 72 0.53 21.3 1.3\u00d7\nMARL 800k 60 0.62 22.1 1.5\u00d7\nMamba 650k 52 0.71 23.4 1.8\u00d7\nMAPPO 550k 45 0.78 23.9 2.2\u00d7\nOurs 380k 38 0.89 24.6 3.1\u00d7\nTable 4. Training convergence comparison. Bold font highlights the performance values achieved by the \nproposed algorithm.\n \nScientific Reports |        (2025) 15:30567 7| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3202, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "17cf43bb-a7a0-4f6c-b9ee-4f19fd8fcb38": {"__data__": {"id_": "17cf43bb-a7a0-4f6c-b9ee-4f19fd8fcb38", "embedding": null, "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56b5ecd5-0897-49df-ae98-02ba53821a79", "node_type": "4", "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "c98557cc0495b02d6a28f424a7a36b2d609c3382010d5ef04ff005bcae14d065", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95f9264f-1223-4231-b66f-e111ea111551", "node_type": "1", "metadata": {}, "hash": "372ff443ed75654b087fb89dce80bf975705fcd87351e37264ebbd2a35bef8d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The largest performance degradation occurred upon removal of the variance-regularized credit assignment \nmechanism, resulting in a 6.7% drop in SLA compliance due to increased instability in the reward signal among \nagents. Similarly, the biLSTM proved essential, contributing 5.4% to SLA performance by enabling forward and \nbackward temporal context for early spike detection.\nThe attention mechanism, while less impactful than the core processing or credit components, still accounted \nfor a meaningful 3.2% improvement by helping the model focus on temporally critical features. The prioritized \nexperience replay improved convergence efficiency, reducing training steps required to converge to 570k \ncompared to slower learning without it.\nCarbon masking, though contributing the smallest performance uplift (2.3%), significantly reduced energy \nconsumption by 15%, justifying its inclusion for sustainable deployment with negligible QoS tradeoff.\nDiscussion\nAll components demonstrated statistically significant contributions to overall system performance ( p < \n0.01 via paired t-tests). Notably, the combination of BiLSTM and credit regularization alone contributed over \n10% to SLA compliance, affirming their critical roles in the architecture. Furthermore, the inclusion of carbon \nmasking supports green AI initiatives, highlighting a tradeoff-aware design strategy that balances performance \nand energy efficiency.\nExperimental Setup for Reproducibility\nAll ablation experiments were conducted on the Azure 2021 trace dataset across a 1,000-node simulated \nenvironment. Each configuration was trained for 200,000 steps under identical conditions to ensure fair \ncomparison.\nOperational economics\nBaseline Comparison Table\u00a06 compares cost metrics across six methods for 10,000-VM deployment.\nQuantitative Results LSTM-MARL-Ape-X achieved:\n\u2022 2.7-month ROI (22% faster than MAPPO, 67% faster than TAS)\n\u2022 24% energy cost share (7.7% reduction vs Mamba, 31% vs TAS)\n\u2022 $59k monthly OpEx (3.3% lower than MAPPO, 30.6% vs TAS)\n\u2022 $98k annual penalties (31% reduction vs MAPPO, 76% vs TAS)Key Improvements\n\u2022 Carbon-aware VM placement: Saved $126k/year in energy costs (18% better than Mamba)\n\u2022 Predictive scaling: Reduced overprovisioning waste by 39% versus MAPPO\n\u2022 Variance-regulated policies: Cut SLA penalties by $44k/year vs best baseline\n\u2022 Distributed control: Lowered coordination overhead costs by 28%Discussion The framework demonstrates:\n\u2022 CapEx/OpEx tradeoff: 5-7% higher initial investment than TAS yields 3\u00d7 faster ROI\n\u2022 Sustainability premium: Carbon-aware decisions add <1% to CapEx but save 18% energy costs\n\u2022 Scalability economics: Maintains linear cost growth at scale (vs quadratic for TFT+RL)Reproducibility De-\ntails\n\u2022 Pricing: AWS EC2 (m5.2xlarge @ $0.384/hr), 80% utilization\n\u2022 Energy: $0.12/kWh (US average), carbon-aware regions @ $0.14/kWh\n\u2022 Penalties: $5k/violation (enterprise SLA terms)\n\u2022 Modeling: 3-year TCO with 5% annual discount rate\nDiscussion\nThe results of this study demonstrate that LSTM-MARL-Ape-X significantly improves cloud resource allocation \nby integrating workload forecasting, decentralized multi-agent coordination, and sample-efficient distributed \ntraining. In contrast to traditional single-agent reinforcement learning (RL) methods such as DQN\u2014which \noften face centralization bottlenecks and reactive behaviors\u2014our framework enables proactive, scalable, and \nenergy-efficient decision-making.\nOur BiLSTM-based workload forecaster outperforms state-of-the-art models such as the Temporal Fusion \nTransformer (TFT) in both accuracy and inference speed. This improvement is attributed to its bidirectional \narchitecture and feature-wise attention mechanism, which together capture long-range temporal dependencies \nwhile maintaining low computational overhead.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95f9264f-1223-4231-b66f-e111ea111551": {"__data__": {"id_": "95f9264f-1223-4231-b66f-e111ea111551", "embedding": null, "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56b5ecd5-0897-49df-ae98-02ba53821a79", "node_type": "4", "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "c98557cc0495b02d6a28f424a7a36b2d609c3382010d5ef04ff005bcae14d065", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17cf43bb-a7a0-4f6c-b9ee-4f19fd8fcb38", "node_type": "1", "metadata": {"page_label": "8", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "59ac4414a21db24ae2ab7a5d4e871b5810000646e8b05689b7566f412e3f951b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast to traditional single-agent reinforcement learning (RL) methods such as DQN\u2014which \noften face centralization bottlenecks and reactive behaviors\u2014our framework enables proactive, scalable, and \nenergy-efficient decision-making.\nOur BiLSTM-based workload forecaster outperforms state-of-the-art models such as the Temporal Fusion \nTransformer (TFT) in both accuracy and inference speed. This improvement is attributed to its bidirectional \narchitecture and feature-wise attention mechanism, which together capture long-range temporal dependencies \nwhile maintaining low computational overhead. The incorporation of quantile regression enhances robustness \nMethod CapEx ($) OpEx ($/mo) Energy Cost Share SLA Penalties (k$/yr) ROI (months)\nTAS 1.2M 85k 38% 412 8.2\nDQN 1.4M 72k 32% 228 6.7\nTFT+RL 1.6M 68k 29% 195 5.9\nMamba 1.55M 63k 27% 168 4.3\nMAPPO 1.52M 61k 26% 142 3.5\nOurs 1.5M 59k 24% 98 2.7\nTable 6. Operational cost analysis (3-year TCO). Bold font highlights the performance values achieved by the \nproposed algorithm.\n \nScientific Reports |        (2025) 15:30567 8| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 3212, "end_char_idx": 4373, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "11d7f0ea-9ce5-4e9e-b816-212c6f637cff": {"__data__": {"id_": "11d7f0ea-9ce5-4e9e-b816-212c6f637cff", "embedding": null, "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c61277e-f05c-4a35-ac72-b22b46efa6cb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "94d110e6d92f31691227abcca0d01a8df9e783daeae083e01ae7bbea290dec07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6a7040a-3483-4861-97ee-a3fc9febeff9", "node_type": "1", "metadata": {}, "hash": "8b1c3500767941bb31eb48ac67472ee12f2e942c8ca730526954b049943a7c1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "under uncertainty, enabling the system to dynamically adapt to sudden traffic spikes\u2014a critical requirement for \nreal-time auto-scaling.\nA core innovation of this work lies in the integration of Multi-Agent Reinforcement Learning (MARL), \nenabling decentralized coordination without compromising control precision. While traditional MARL \nframeworks often face challenges with reward attribution and scalability, our variance-regularized credit \nassignment mechanism stabilizes learning across thousands of agents, reducing SLA violations by 72% compared \nto centralized RL baselines. This confirms that decentralized coordination can scale linearly while maintaining \nhigh performance\u2014overcoming a major limitation in previous transformer-based and Ape-X approaches.\nAdditionally, our enhanced Ape-X architecture with uncertainty-aware prioritized replay significantly \naccelerates convergence. By factoring forecast uncertainty into the priority calculation, the learner is guided \ntoward high-impact transitions, achieving a 3.2 \u00d7 faster training time than uniform sampling. This makes the \nframework more suitable for dynamic production environments, where rapid adaptation is essential.\nOur economic and sustainability analysis further highlights practical benefits. The framework reduces energy \nconsumption by 22% through carbon-aware VM placement and minimizes operational costs via reduced over-\nprovisioning and SLA penalties. With a return-on-investment (ROI) period of just 2.7 months in large-scale \ndeployments, the proposed approach offers substantial value for enterprise cloud providers seeking to meet both \nservice-level agreements and green computing goals.\nDespite these advantages, certain limitations persist. The current implementation assumes relatively \nhomogeneous workloads, which may constrain its applicability in heterogeneous environments such as \nmicroservices or serverless architectures. Moreover, while the BiLSTM forecaster performs well on periodic \nand semi-periodic workloads, it may require retraining or fine-tuning to maintain accuracy in the presence of \npersistent structural shifts in demand patterns.\nFuture work will aim to extend the framework to support diverse workloads, including containerized \nservices and edge computing scenarios. We also plan to incorporate explainability features to enhance decision \ntransparency and to explore federated learning strategies for preserving data privacy across distributed \ninfrastructures. Finally, we intend to integrate hardware-aware adaptation mechanisms to optimize performance \nacross heterogeneous compute resources such as GPUs and TPUs.\nIn conclusion, LSTM-MARL-Ape-X represents a novel end-to-end solution for intelligent cloud orchestration. \nBy unifying forecasting, policy learning, and resource optimization, the proposed system outperforms traditional \ndecoupled prediction-action pipelines, offering robust, scalable, and sustainable resource management at cloud \nscale\u2014an essential capability for next-generation platforms.\nMethods\nDataset description\nTo validate the robustness of our framework, experiments were conducted on multiple widely-used real-world \nand synthetic cloud workload datasets:\n\u2022 Google Cluster Trace: A large-scale production trace from Google containing resource usage information in \na data set of more than 12,000 machines for one month\u00a026. This dataset includes granular metrics such as CPU, \nmemory, disk I/O, and network utilization, recorded at 5-minute intervals.\n\u2022 Microsoft Azure Trace: Publicly available data capturing diverse Azure VM workloads. It includes metrics \nsuch as CPU, memory, and network usage, sampled every 5 minutes\u00a027.\n\u2022 Bitbrains Synthetic Dataset: Simulates bursty and seasonal workload patterns typically observed in enter -\nprise cloud environments, enabling controlled evaluation of model adaptability under dynamic conditions\u00a028.\nData preprocessing\nPrior to model training and inference, a structured data preprocessing pipeline is applied to ensure high-quality \nand consistent input:\n\u2022 Normalization: All workload metrics are scaled to the range [0, 1] using min-max normalization to promote \nstable neural network training and prevent feature dominance due to varying scales.\n\u2022 Missing Value Imputation: Missing or corrupted entries are addressed by linear interpolation, maintaining \ntemporal continuity.\n\u2022 Windowing: For time-series forecasting models (e.g. LSTM, BiLSTM, TFT), input sequences are constructed \nusing a sliding window with fixed historical length L and prediction horizon H.\n\u2022 Feature Engineering: Each time step is represented by 23 system-level features, as summarized in Table\u00a07.\n\u2022 Train/Validation/Test Split: Datasets are partitioned using a ratio 70% / 15% / 15% for training, validation \nand testing, ensuring unbiased model evaluation and effective hyperparameter tuning.\n\u2022 Workload Aggregation: Depending on the evaluation scenario, data may be aggregated at varying granular-\nities (e.g., hourly, every 5 minutes) to simulate different operational conditions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b6a7040a-3483-4861-97ee-a3fc9febeff9": {"__data__": {"id_": "b6a7040a-3483-4861-97ee-a3fc9febeff9", "embedding": null, "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7c61277e-f05c-4a35-ac72-b22b46efa6cb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "94d110e6d92f31691227abcca0d01a8df9e783daeae083e01ae7bbea290dec07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11d7f0ea-9ce5-4e9e-b816-212c6f637cff", "node_type": "1", "metadata": {"page_label": "9", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "3d07449a5586c8cfe2fe7d48c0a7420fd8a59968b2100721e5e14ca00b77da09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Missing Value Imputation: Missing or corrupted entries are addressed by linear interpolation, maintaining \ntemporal continuity.\n\u2022 Windowing: For time-series forecasting models (e.g. LSTM, BiLSTM, TFT), input sequences are constructed \nusing a sliding window with fixed historical length L and prediction horizon H.\n\u2022 Feature Engineering: Each time step is represented by 23 system-level features, as summarized in Table\u00a07.\n\u2022 Train/Validation/Test Split: Datasets are partitioned using a ratio 70% / 15% / 15% for training, validation \nand testing, ensuring unbiased model evaluation and effective hyperparameter tuning.\n\u2022 Workload Aggregation: Depending on the evaluation scenario, data may be aggregated at varying granular-\nities (e.g., hourly, every 5 minutes) to simulate different operational conditions.\nEvaluation metrics\nTo assess the effectiveness of our workload forecasting and resource allocation mechanisms, we adopt multiple \nperformance indicators spanning accuracy, efficiency, cost, and sustainability:\n\u2022 Mean Absolute Error (MAE): Represents the average magnitude of prediction errors, independent of direc-\ntion. Lower MAE indicates better forecasting performance.\n\u2022 Root Mean Squared Error (RMSE): Penalizes larger errors more significantly than MAE, providing a meas-\nure of model robustness.\nScientific Reports |        (2025) 15:30567 9| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 4246, "end_char_idx": 5685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "af88c077-f8b9-4f08-8c0b-5a4f8acb0034": {"__data__": {"id_": "af88c077-f8b9-4f08-8c0b-5a4f8acb0034", "embedding": null, "metadata": {"page_label": "10", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3759b0c6-b6cf-4ec9-86fc-38c15ca3bd34", "node_type": "4", "metadata": {"page_label": "10", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "790e6ebddbb06e1fb209873b5b563026661a1af2ca7ba6873a2b88e363fc6851", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Mean Absolute Percentage Error (MAPE): Expresses errors as a percentage of actual values, making it suit-\nable for relative comparisons across different scales.\n\u2022 Scaling Efficiency (SE): Defined as the ratio of allocated resources to actual usage. An SE close to 1 indicates \noptimal resource provisioning with minimal under- or over-allocation.\n\u2022 SLA Violation Rate: Measures the proportion of time steps where resource provisioning fails to meet appli-\ncation demand. Lower values indicate more reliable system behavior.\n\u2022 Energy Consumption: Computed based on CPU-hours and cloud-specific energy models. We also include \ncarbon-aware metrics derived from energy-efficient scheduling practices.\n\u2022 Cost Savings: Based on Amazon Web Services Elastic Compute Cloud AWS EC2 pricing, this metric quanti-\nfies the monetary benefits of dynamic and intelligent scaling strategies.\nTogether, these metrics offer a holistic view of model performance across predictive accuracy, operational \nefficiency, reliability, energy sustainability, and economic cost.\nBaseline models\nTo evaluate the performance of our proposed BiLSTM-MARL-Ape-X framework, we compare it against a \ndiverse and well-established set of baselines across three core areas: workload prediction, resource allocation, \nand training optimization.\nWorkload Prediction. We consider both classical and deep learning-based models for time-series forecasting:\n\u2022 ARIMA\u00a029: A classical ARIMA model used for modeling linear time-series data.\n\u2022 LSTM\u00a030: LSTM network widely adopted for capturing long-range dependencies in sequential data.\n\u2022 TFT (Temporal Fusion Transformer)\u00a031: A transformer-based model that integrates attention mechanisms \nand interpretable temporal features for robust forecasting.Resource Allocation. We evaluate RL and heuris-\ntic-based baselines for dynamic resource scaling:\n\u2022 TAS (Threshold Auto-Scaling): A widely used rule-based reactive mechanism that scales resources based on \npredefined thresholds.\n\u2022 DQN \u00a032: A RL algorithm that uses Q-learning for resource management in dynamic environments.\n\u2022 TFT+RL: A hybrid approach that couples Temporal Fusion Transformer for forecasting with RL for deci -\nsion-making.\n\u2022 MARL \u00a033: A scalable method utilizing multiple decentralized agents for cooperative or competitive environ-\nments.Training Optimization. For scalable and efficient policy learning, we incorporate:\nCategory Feature Description\nCPU\nUsage (%) Total CPU utilization across all cores\nFrequency (GHz) Average processor clock speed\nTemperature (\u25e6C) Processor package temperature\nCore Variance Standard deviation of core usage\nMemory\nUtilization (%) RAM usage percentage\nCache Misses (k/sec) L3 cache miss rate\nSwap Usage (GB) Swap memory in use\nPage Faults (/sec) Rate of page faults\nDisk\nRead Rate (MB/s) Disk read throughput\nWrite Rate (MB/s) Disk write throughput\nInput/Output Operations Per Second IOPS Input/output operations per second\nLatency (ms) Average disk I/O delay\nNetwork\nInbound (Mbps) Incoming network traffic\nOutbound (Mbps) Outgoing network traffic\nPacket Loss (%) Packet drop rate\nTCP Errors Count of TCP-related errors\nEnergy\nPower (W) Instantaneous power draw\nEnergy (kWh) Cumulative energy consumption\nPower Usage Effectiveness(PUE) Ratio of total facility energy to IT equipment energy\nSystem\nContext Switches Number of process switches per second\nInterrupts Hardware interrupt rate\nLoad Average 1-minute system load average\nTemporal\nHour of Day Encoded cyclically from 0\u201323\nDay of Week Encoded cyclically from 0\u20136\nMinute of Hour Normalized from 0\u201359\nTable 7. List of 23 Input Features per Time Step.\n \nScientific Reports |        (2025) 15:30567 10| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "008ca87d-10f8-425d-84ca-769bdb2e1fe5": {"__data__": {"id_": "008ca87d-10f8-425d-84ca-769bdb2e1fe5", "embedding": null, "metadata": {"page_label": "11", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d196a493-9b3d-495d-a7a4-286a43981d93", "node_type": "4", "metadata": {"page_label": "11", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "d67f04699ec9e2783394c00db263308ac4090ea0c4a5a42f154584b1ae43148f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Ape-X\u00a014: A distributed architecture for RL that leverages prioritized experience replay and asynchronous \nlearners to accelerate training.\nThese baselines offer a comprehensive benchmarking foundation for assessing the contributions of each module \nwithin our proposed framework.\nProposed framework\nThis section describes our proposed LSTM-MARL-Ape-X framework designed for intelligent, carbon-aware \nauto-scaling in cloud environments. The framework integrates three core components: (1) a BiLSTM-based \nworkload forecaster, (2) (MARL) decision engine, and (3) a distributed experience replay mechanism inspired \nby Ape-X.\nWorkload forecasting using BiLSTM\nTo accurately model temporal dependencies in cloud workloads, we propose a (BiLSTM) network enhanced \nwith an attention mechanism and quantile regression output. As illustrated in Table\u00a0 8, the model processes \nsequences bidirectionally (forward and backward), capturing both past and future context critical for volatile, \nbursty workload patterns\u00a034.\nArchitectural Advantages Compared to transformer-based models\u00a035, our BiLSTM design offers:\n\u2022 Higher computational efficiency for edge deployment\n\u2022 Lower inference latency (critical for real-time scaling)\n\u2022 Fewer trainable parameters (reduced overfitting risk)\nUncertainty-Aware Training The model ingests one hour of historical metrics (12 timesteps) and predicts three \nquantiles using the pinball loss function\u00a036:\n \nL\u03c4 (y, \u02c6y)=\n{ \u03c4 \u00b7| y \u2212 \u02c6y| if y \u2265 \u02c6y,\n(1 \u2212 \u03c4 ) \u00b7| y \u2212 \u02c6y| otherwise.  (1)\nwhere \u03c4 \u2208{ 0.1, 0.5, 0.9}. The median (50%) serves as the point forecast, while the interquartile range (10%\u2013\n90%) informs robust autoscaling policies under uncertainty.\nReinforcement learning-based auto-scaling\nOur Multi-Agent Reinforcement Learning (MARL) system deploys distributed agents, each managing a subset \nof virtual machines (VMs) with shared objectives. As detailed in Table\u00a0 9, agents observe a hybrid state space \ncombining forecasts from Section\u00a05.5.1 with real-time operational metrics.\nPolicy Architecture Each agent implements a continuous control policy \u03c0\u03b8 with:\n\u2022 Action space a \u2208 [\u22121, 1]3: \nFeature Category Description\nForecasted Load BiLST M\u2212 predicted workload quantiles(10/50/90%)\nCurrent Utilization Normalized CP U/memory usage[0, 1]\nVM Status One \u2212 hot encoded: {active, paused, suspended}\nCarbon Intensity Real \u2212 time grid emission factor (gCO2/kWh)\nQueue Length P ending jobs(normalized by cluster capacity)\nEnergy Budget Remaining renewable quota(% of daily limit)\nTable 9. Agent Observation Space Composition.\n \nComponent Configuration\nInput sequence length 12\nFeature dimension 23\nBiLSTM layers 2\nHidden units/layer 64\nAttention mechanism Temporal softmax\nOutput quantiles [10%, 50%, 90%]\nTable 8. BiLSTM Forecaster Architecture.\n \nScientific Reports |        (2025) 15:30567 11| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3e99a89a-1972-4655-ac8b-f0f0a3c60c62": {"__data__": {"id_": "3e99a89a-1972-4655-ac8b-f0f0a3c60c62", "embedding": null, "metadata": {"page_label": "12", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c430f613-40b9-478d-9c88-9db38a9ebe26", "node_type": "4", "metadata": {"page_label": "12", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "cdd9e1c43757a1b743c3e52aa64753928d7904f45657a1615c4064d2fa2c79f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "at =\n[\nascale\ued19 \ued18\ued17 \ued1a\nScaling\nratio\n,a migrate\n\ued19 \ued18\ued17 \ued1a\nVM migration\npriority\n,a suspend\n\ued19 \ued18\ued17 \ued1a\nSuspension\nthreshold\n]\n (2)\n\u2022 Carbon-aware action masking: We implement soft constraints to suppress high-emission actions using: \n mask =\n{ 0 if ct >\u03c4 carbon\n1 otherwise  (3)\n where \u03c4carbon = 500 gCO2/kWh is the emission threshold determined through empirical analysis of our cloud \ninfrastructure. This value represents the 90th percentile of historical carbon intensity values in our deployment \nregion.\n\u2022 Exploration strategy: We employ Ornstein-Uhlenbeck noise (\u03b8 =0 .15, \u03c3 =0 .2) for temporally correlated \nexploration, which provides smoother action sequences compared to uncorrelated noise for resource alloca-\ntion tasks.Multi-Objective Reward Design The reward function integrates four key components:\n \nrt = \u2212\u03b1 \u00b7 \u2113t\ued19 \ued18\ued17 \ued1a\nPerformance\n\u2212 \u03b2 \u00b7 ct\n\ued19\ued18\ued17\ued1a\nSustainability\n+ \u03b3 \u00b7 ut\n\ued19 \ued18\ued17 \ued1a\nE\ufb03ciency\n+\u03bb \u00b7 Crediti\ued19 \ued18\ued17 \ued1a\nStabilization\n (4)\nwhere the variance-regularized credit assignment for agent i is computed as:\n \nCrediti = ri\n\u03c32\ni + \u03f5 \u00b7 I(\u03c32\ni <\u03c4 v) (5)\nThe components are defined as:\n\u2022 \u2113t: 95th percentile request latency (normalized to [0,1])\n\u2022 ct: Carbon emissions from Equation\u00a06 (gCO2/kWh)\n\u2022 ut: Weighted resource utilization (CPU 40%, memory 40%, GPU 20%)\n\u2022 ri: Immediate reward for agent i\n\u2022 \u03c32\ni : Reward variance over a 100-step moving window\n\u2022 \u03f5 = 10\u22125: Numerical stability constant\n\u2022 \u03c4v =0 .1: Variance threshold for stable learning\n\u2022 I(\u00b7): Indicator function (1 if condition holds, 0 otherwise)\nThe weighting coefficients ( \u03b1 =0 .5, \u03b2 =0 .3, \u03b3 =0 .2, \u03bb =0 .1) were optimized through multi-objective \nBayesian optimization\u00a037. Our credit assignment mechanism provides three key benefits:\n\u2022 Variance penalization: Agents with unstable learning behavior (\u03c32\ni  > \u03c4v) receive reduced credit\n\u2022 Magnitude scaling: Well-performing agents are proportionally rewarded\n\u2022 Stability guarantee: The \u03c4v threshold completely disables credit for extremely unstable agents\n \nct =\nK\u2211\nk=1\n(\nCI(k)\ngrid \u00b7 P(k)\nVM + CI(k)\ndiesel \u00b7 B(k)\nusage\n)\n (6)\nwhere K is the number of energy sources, CI represents carbon intensity, PVM is VM power consumption, and \nBusage is backup generator usage.\nApe-X distributed training architecture\nWe implement a modified Ape-X framework\u00a014 that combines distributed experience collection with uncertainty-\naware prioritization. As shown in Table\u00a010, the system leverages:\n\u2022 Parallel actors (32 instances) generating diverse trajectories\nComponent Value\nActors 32\nLearners 8\nReplay Buffer 1M transitions\nPriority (\u03b1) 0.6\nUncertainty Metric \u03c3BiLSTM\nSample Interval 4 steps\nTable 10. Ape-X Distributed Training Configuration.\n \nScientific Reports |        (2025) 15:30567 12| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7548b177-bd98-4b89-abec-255c876b6e03": {"__data__": {"id_": "7548b177-bd98-4b89-abec-255c876b6e03", "embedding": null, "metadata": {"page_label": "13", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7d1dc42-eaae-4bda-bc97-0316f6ade11d", "node_type": "4", "metadata": {"page_label": "13", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "d155280f28424dfb37bde4e38967225073c09ca6e6203424931b27cd75239937", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Decoupled learners (8 GPUs) performing prioritized updates\n\u2022 Forecast-guided sampling using BiLSTM uncertainty estimates\nUncertainty-Aware Prioritization Building on\u00a012, we compute sample priority as:\n pi = |\u03b4i|\u03b1 + \u03bb\u03c3BiLSTM(si) (7)\nwhere \u03b4i is TD-error and \u03bb =0 .3 controls uncertainty weighting.\nIntegrated LSTM-MARL-Ape-X algorithm\nThe proposed LSTM-MARL-Ape-X framework unifies time series forecasting, intelligent scaling, and distributed \ntraining into a single pipeline for carbon-aware and efficient auto-scaling. The system operates in continuous \ncycles of forecasting, decision-making, and learning. The complete workflow is described below.\nSystem Workflow\n 1. Data Collection and Preprocessing:  Metrics such as Central Processing Unit CPU usage, memory con -\nsumption, job queue length, carbon intensity, and resource state are collected every 5 minutes. Each sample \nis normalized using z-score normalization. Synthetic rare-load scenarios are generated using a Wasserstein \nGenerative Adversarial Network GAN to enrich training data.\n 2. Forecasting with BiLSTM: A BiLSTM model with attention is used to predict three quantiles (10%, 50%, \n90%) of the future workload based on a sliding window of the last 12 timesteps (one hour). The model out -\nputs probabilistic forecasts that help account for uncertainty.\n 3. Agent Observation: Each RL agent receives a local observation that includes forecasted load, real-time sys-\ntem state (CPU, memory, queue), carbon intensity, and green energy budget.\n 4. Action Selection: Each agent outputs a continuous action vector \n a =[ scale, migrate, suspend]\n  constrained to the range [\u22121, 1]. A soft mask is applied to discourage actions that increase carbon usage unnec-\nessarily.\n 5. Environment Execution: The environment executes the agents\u2019 actions, updates the system state, and re -\nturns a reward \n r = \u2212\u03b1 \u00b7 latency \u2212 \u03b2 \u00b7 carbon + \u03b3 \u00b7 utilization\n  balancing performance and sustainability.\n 6. Ape-X Training: Each agent\u2019s transition is stored in a shared prioritized replay buffer. Priority is influenced \nby forecast uncertainty (standard deviation of predicted quantiles). Learners sample high-priority transitions \nfor gradient updates. Multiple actors and learners enable scalable asynchronous training.\n 7. Policy Update and Execution Loop: Trained policy weights are distributed back to actors periodically. The \nsystem continues to learn and adapt in real time as the environment evolves.Pseudocode\nScientific Reports |        (2025) 15:30567 13| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8b576b3a-a919-47ed-b6e6-a1e30dc8ea2c": {"__data__": {"id_": "8b576b3a-a919-47ed-b6e6-a1e30dc8ea2c", "embedding": null, "metadata": {"page_label": "14", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "311a0137-d750-4212-95fd-c30012c0de18", "node_type": "4", "metadata": {"page_label": "14", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "efdfb1026a2f53353171a6f14b5090efb807be6c59a274a69c701b92ce16b7f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Algorithm 1.  LSTM-MARL-Ape-X Algorithm \nThis algorithm extends ideas from prior work on RL with prioritized experience replay 14 and time series \nforecasting with BiLSTM models30. The carbon-aware masking strategy is inspired by recent advances in green \nAI38.\nImplementation details\nOur implementation unifies forecasting, resource management, and training optimization within a single auto-\nscaling framework. Key components include:\n\u2022 Workload Forecasting: Models include ARIMA, LSTM, BiLSTM with attention (our proposed variant), and \nTFT. Hyperparameters are tuned via Bayesian Optimization using historical workload data.\n\u2022 Reinforcement Learning: We implement DQN, MARL, and our proposed LSTM-MARL-Ape-X, which inte-\ngrates distributed prioritized experience replay (Ape-X) and adaptive credit assignment.\n\u2022 Training Environment: All components are developed using Python with PyTorch and TensorFlow. RL \nmodels are implemented using RLlib with custom extensions for distributed training.\n\u2022 Optimization: Bayesian Optimization is applied to fine-tune hyperparameters. We use quantile regression \nand variance-regularized credit assignment to enhance stability and uncertainty estimation.\n\u2022 Energy Efficiency:  Carbon-aware action masking is incorporated to guide environment-friendly resource \nscheduling decisions.\n\u2022 Hardware Setup: Experiments are run on Google Cloud Platform (n1-standard-16) VMs with 16 vCPUs \nand 60 GB RAM. Results are averaged over five trials with distinct random seeds to ensure statistical validity.\nThe source code and configuration scripts will be made publicly available upon acceptance to facilitate \nreproducibility and future research.\nScientific Reports |        (2025) 15:30567 14| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "d411ca0b-03db-4cb4-bf84-3d400df92f37": {"__data__": {"id_": "d411ca0b-03db-4cb4-bf84-3d400df92f37", "embedding": null, "metadata": {"page_label": "15", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3625476a-13ad-4c5f-b0a8-851eb769b72a", "node_type": "4", "metadata": {"page_label": "15", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "52e2da491170b931af258dea58432f67799c8ed4dda95e44f49bf5539c39bad3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Training strategy and reproducibility\nTo ensure full reproducibility and transparency, we present the training configurations of all major components \nin Table\u00a011, Table\u00a012, and Table\u00a013.\nReproducibility Measures\n\u2022 70/15/15 train/validation/test split maintained across all experiments\n\u2022 Results averaged over 5 different random seeds\n\u2022 Implemented in PyTorch, TensorFlow, and Ray RLlib (custom Ape-X)\n\u2022 Hardware: Google Cloud (n1-standard-16 VMs), Tesla V100 GPUs\nEvaluation methodology\nWe adopt a rigorous evaluation strategy to ensure robust and generalizable conclusions.\n\u2022 Data Splitting: A 70/15/15 train/validation/test split is used to evaluate the learning, tuning, and generaliza-\ntion phases.\n\u2022 Stress Testing: A 24-hour stress test is conducted to simulate high-load, real-world scenarios and assess the \nresilience of the system.\nParameter Value\nNumber of Actors 32 parallel agents\nNumber of Learners 8 (GPU-distributed)\nPriority Sampling pi = |\u03b4i|0.6 +0 .3 \u00b7 \u03c3BiLSTM(si)\nTarget Update Soft update with \u03c4 =0 .005\nSample Interval Every 4 environment steps\nGradient Clipping Max norm = 10\nTable 13. Training Configuration for Ape-X Learners.\n \nParameter Value\nPolicy Network 3 hidden layers (128, 128, 64), ReLU\nAction Space Continuous [\u22121, 1]3 (scale, migrate, suspend)\nReward Weights \u03b1 =0 .5, \u03b2 =0 .3, \u03b3 =0 .2\nOptimizer Adam (initial lr = 5 \u00d7 10\u22124, linear decay)\nDiscount Factor \u03b3 =0 .99\nExploration Strategy Ornstein\u2013Uhlenbeck (\u03b8 =0 .15, \u03c3 =0 .2)\nReplay Buffer Size 1 million transitions\nBatch Size 512\nCredit Assignment Variance-Regularized\nTable 12. Training Configuration for MARL Agents.\n \nParameter Value\nModel Architecture 2-layer BiLSTM\nHidden Units per Layer 64\nAttention Mechanism Temporal Softmax\nLoss Function Pinball Loss (\u03c4 \u2208{ 0.1, 0.5, 0.9})\nOptimizer Adam (lr =0 .001, \u03b21 =0 .9, \u03b22 =0 .999, \u03f5 = 10\u22128)\nLearning Scheduler Cosine annealing with 5-epoch warm-up\nRegularization Dropout (rate = 0.3)\nBatch Size 256\nMax Epochs 100\nEarly Stopping Patience = 10 (based on validation MAE)\nInput Normalization Z-score\nTuning Method Bayesian Optimization (50 trials)\nTable 11. Training Configuration for BiLSTM Forecaster.\n \nScientific Reports |        (2025) 15:30567 15| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "55dfef50-accd-4878-9f0c-8bd1720f9035": {"__data__": {"id_": "55dfef50-accd-4878-9f0c-8bd1720f9035", "embedding": null, "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "b7bec4febcd88896d16c543908751f76efcfe991ace911165a4f49f7d53f4ce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d", "node_type": "1", "metadata": {}, "hash": "cf7063a247211e383cd87ba94f2113bfcb2714e695e83a5f68af9f1c5b56af2a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u2022 Deployment Environment: Experiments are deployed on Google Cloud Platform GCP instances to mimic \nreal-world infrastructure setups.\n\u2022 Cost Analysis: An economic evaluation is performed using the AWS EC2 pricing to analyze cost-effective -\nness.\nData availability\nThe datasets used to evaluate the proposed framework are publicly available and can be accessed as follows:\n\u2022 Google Cluster Trace: Available at https://github.com/google/cluster-data. This dataset contains resource \nusage traces from Google\u2019s production clusters, including CPU, memory, and disk usage over time.\n\u2022 Azure Public Dataset: Available at https://github.com/Azure/AzurePublicDataset. This dataset includes \nVM workload traces from Microsoft Azure, capturing resource utilization metrics such as CPU, memory, and \nnetwork I/O.\n\u2022 Bitbrains Trace: Available at https://github.com/bitbrains. This dataset contains performance metrics \nfrom enterprise-level cloud workloads, including CPU utilization, memory usage, and disk I/O.\nThese datasets were preprocessed and normalized for use in our experiments. The preprocessing scripts and \ndetailed instructions for reproducing the results are available in our GitHub repository  h t t p s : / / g i t h u b . c o m / f a d y \nn a s h a t / L S T M M A R L A P e - x _ S o l /     .  \nReceived: 26 February 2025; Accepted: 4 August 2025\nReferences\n 1. Zhang, L., Chen, W . & Wang, H. Deep q-networks for cloud resource allocation: Challenges and opportunities. IEEE Trans. Cloud \nComput. 11, 145\u2013160 (2023).\n 2. Li, Y ., Liu, J. & Zhang, Q. Centralized vs. decentralized reinforcement learning for cloud resource management. ACM SIGMETRICS \nPerformance Evaluation Review49, 45\u201350 (2022).\n 3. Alharthi, S., Alshamsi, A., Alseiari, A. & Alwarafy, A. Auto-scaling techniques in cloud computing: Issues and research directions. \nSensors 24, 5551. https://doi.org/10.3390/s24175551 (2024).\n 4. Lim, B., Ar\u0131k, S., Loeff, N. & Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. Int. J. \nForecast. 37, 1748\u20131764. https://doi.org/10.1016/j.ijforecast.2021.03.012 (2021).\n 5. Bernstein, D., Wang, Y . & Pan, S. Distributed reinforcement learning for scalable cloud resource management. J. Artif. Intell. Res. \n74, 1023\u20131060 (2022).\n 6. Ali, T., Khan, H. U., Alarfaj, F . & Alreshoodi, M. Hybrid deep learning and evolutionary algorithms for accurate cloud workload \nprediction. Computing 106, 3905\u20133944. https://doi.org/10.1007/s00607-024-01340-8 (2024).\n 7. Microsoft Azure Team. Azure workload traces and analysis. Tech. Rep., Microsoft Corporation (2022).\n 8. Y oung, P . C. & Shellswell, S. Time series analysis, forecasting and control. IEEE Trans. Autom. Control 17, 281\u2013283.  h t t p s : / / d o i . o r g \n/ 1 0 . 1 1 0 9 / T A C . 1 9 7 2 . 1 0 9 9 9 6 3     (1972).\n 9. Singh, S., Tiwari, M. & Dhar, A. Machine learning based workload prediction for auto-scaling cloud applications. In 2022 OPJU \nInternational Technology Conference on Emerging Technologies for Sustainable Development (OTCON), 1\u20136,  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 \n9 / O T C O N 5 6 0 5 3 . 2 0 2 3 . 1 0 1 1 4 0 3 3     (2023).\n 10. Nguyen, T. et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d": {"__data__": {"id_": "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d", "embedding": null, "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "b7bec4febcd88896d16c543908751f76efcfe991ace911165a4f49f7d53f4ce7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55dfef50-accd-4878-9f0c-8bd1720f9035", "node_type": "1", "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "796c16e5185d82ec07ad325ecbb9efaad62c771722df6773e2b83f35384fc0cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a53677d0-13bb-4a05-94e0-20cbba1a835d", "node_type": "1", "metadata": {}, "hash": "68dbfff84c4ae5f7726b504af88b8ff01eba488b4b95f39c9a7c2738b7b4e162", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 1 0 9 / T A C . 1 9 7 2 . 1 0 9 9 9 6 3     (1972).\n 9. Singh, S., Tiwari, M. & Dhar, A. Machine learning based workload prediction for auto-scaling cloud applications. In 2022 OPJU \nInternational Technology Conference on Emerging Technologies for Sustainable Development (OTCON), 1\u20136,  h t t p s : / / d o i . o r g / 1 0 . 1 1 0 \n9 / O T C O N 5 6 0 5 3 . 2 0 2 3 . 1 0 1 1 4 0 3 3     (2023).\n 10. Nguyen, T. et al.  An lstm-based approach for predicting resource utilization in cloud computing. In Proceedings of the 11th \nInternational Symposium on Information and Communication Technology , 107\u2013113, https://doi.org/10.1145/3568562.3568647 \n(2022).\n 11. Tay, Y ., Dehghani, M., Bahri, D. & Metzler, D. Efficient transformers: A survey. ACM Comput. Surv. 55, 1\u201328.  h t t p s : / / d o i . o r g / 1 0 . 1 \n1 4 5 / 3 5 3 0 8 1 1     (2020).\n 12. Schaul, T., Quan, J., Antonoglou, I. & Silver, D. Prioritized experience replay. arXiv preprintarXiv:1511.05952 ,  h t t p s : / / d o i . o r g / 1 0 . 4 \n8 5 5 0 / a r X i v . 1 5 1 1 . 0 5 9 5 2     (2015).\n 13. Johnson, M. & Lee, J. Bias in cloud rl: Challenges and mitigations. ACM Trans. Autonom. Adapt. Syst. 16, 1\u201325.  h t t p s : / / d o i . o r g / 1 0 \n. 1 1 4 5 / 3 4 7 3 9 2 1     (2021).\n 14. Horgan, D. et al. Distributed prioritized experience replay. arXiv preprintarXiv:1803.00933 (2018).\n 15. Espeholt, L. et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprintarXiv: \nabs/1802.01561, https://doi.org/10.48550/arXiv.1802.01561 (2018).\n 16. Lorido-Botran, T. & Bhatti, M.\u00a0K. Impalae: Towards an optimal policy for efficient resource management at the edge. Journal of \nEdge Computing1, 43\u201354, https://doi.org/10.55056/jec.572 (2022).\n 17. Foerster, J. N., Farquhar, G., Afouras, T., Nardelli, N. & Whiteson, S. Counterfactual multi-agent policy gradients. Proc. AAAI Conf. \nArtif. Intell. 32, 2974\u20132982. https://doi.org/10.1609/aaai.v32i1.11794 (2018).\n 18. Lowe, R. et al. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing \nSystems30 (2017).\n 19. Taylor, S. & Clark, J. Evaluation of marl methods for cloud resource allocation. IEEE Trans. Cloud Eng. 1, 1\u201315 (2022).\n 20. Anderson, C. & Garcia, M. Multi-objective optimization for cloud resource management. J. Cloud Optim. 3, 78\u201395 (2024).\n 21. Patel, R. & Nguyen, L. Carbon-aware reinforcement learning for sustainable cloud computing. Sustain. Comput.", "mimetype": "text/plain", "start_char_idx": 2778, "end_char_idx": 5276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a53677d0-13bb-4a05-94e0-20cbba1a835d": {"__data__": {"id_": "a53677d0-13bb-4a05-94e0-20cbba1a835d", "embedding": null, "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5039b5b9-4cf4-4b20-ac42-30a9cd08a16f", "node_type": "4", "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "b7bec4febcd88896d16c543908751f76efcfe991ace911165a4f49f7d53f4ce7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f396cff-c8b1-41d8-ae99-34aa6e6ee31d", "node_type": "1", "metadata": {"page_label": "16", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "e2133185bc5d20f4684e8a5059845e9739b4fe35d6516cc79a424ce10595c19a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "AAAI Conf. \nArtif. Intell. 32, 2974\u20132982. https://doi.org/10.1609/aaai.v32i1.11794 (2018).\n 18. Lowe, R. et al. Multi-agent actor-critic for mixed cooperative-competitive environments. Advances in Neural Information Processing \nSystems30 (2017).\n 19. Taylor, S. & Clark, J. Evaluation of marl methods for cloud resource allocation. IEEE Trans. Cloud Eng. 1, 1\u201315 (2022).\n 20. Anderson, C. & Garcia, M. Multi-objective optimization for cloud resource management. J. Cloud Optim. 3, 78\u201395 (2024).\n 21. Patel, R. & Nguyen, L. Carbon-aware reinforcement learning for sustainable cloud computing. Sustain. Comput.: Inf. Syst.  38, \n100876 (2023).\n 22. Wilson, A. & Brown, D. Temporal fusion with rl for cloud workloads. Mach. Learn. Syst. 4, 112\u2013130 (2022).\n 23. Lee, J. & Martinez, C. End-to-end marl for cloud resource management. J. Autonom. Syst. 12, 45\u201367 (2023).\n 24. Harris, M. & Turner, S. Training optimization for cloud rl systems. IEEE Trans. Mach. Learn. 15, 2100\u20132115 (2022).\n 25. Gomez, L. & Schmidt, A. Decentralized coordination for cloud systems. Distribut. AI Rev. 7, 33\u201350 (2023).\n 26. Google Cluster Data. Google cluster trace (2011). Accessed: 2023-10-15.\n 27. Microsoft Azure. Azure public dataset (2019). Accessed: 2023-10-15.\n 28. Bitbrains. Bitbrains cloud workload traces (2020). Accessed: 2023-10-15.\n 29. Box, G. E.\u00a0P ., Jenkins, G.\u00a0M., Reinsel, G.\u00a0C. & Ljung, G.\u00a0M. Time Series Analysis: Forecasting and Control (John Wiley & Sons, \nHoboken, NJ, 2015), 5th edn.\n 30. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735\u20131780 (1997).\n 31. Lim, B., Arik, S. O., Loeff, N. & Pfister, T. Temporal fusion transformers for interpretable multi-horizon time series forecasting. Int. \nJ. Forecast. 37, 1748\u20131764 (2021).\n 32. Mnih, V . et al. Human-level control through deep reinforcement learning. Nature 518, 529\u2013533 (2015).\n 33. Zhang, K., Y ang, Z. & Ba\u015far, T. Multi-agent reinforcement learning: A survey. Found. Trends Mach. Learn. 14, 1\u2013135 (2021).\nScientific Reports |        (2025) 15:30567 16| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 4668, "end_char_idx": 6790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "13a8ff70-6ce4-46b6-b437-e41a0a039f9a": {"__data__": {"id_": "13a8ff70-6ce4-46b6-b437-e41a0a039f9a", "embedding": null, "metadata": {"page_label": "17", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "032e978a-bad2-4e71-a14d-94f679b6a8f1", "node_type": "4", "metadata": {"page_label": "17", "file_name": "input_pdf.pdf", "file_path": "/Users/anik/Desktop/LLm/ChatGPT/data/kb_docs/input_pdf.pdf", "file_type": "application/pdf", "file_size": 3191174, "creation_date": "2025-09-06", "last_modified_date": "2025-08-31"}, "hash": "6572b8f8abc15d972ac4c3b1432232e9cf990b36703527d131fd4458bdfe0ae9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "34. Schuster, M. & Paliwal, K. K. Bidirectional recurrent neural networks. IEEE Trans. Signal Process. 45, 2673\u20132681.  h t t p s : / / d o i . o r g / 1 \n0 . 1 1 0 9 / 7 8 . 6 5 0 0 9 3     (1997).\n 35. Vaswani, A. et al. Attention is all you need. Advances in Neural Information Processing Systems30 (2017).\n 36. Koenker, R. & Hallock, K. F . Quantile regression. J. Econ. Perspect. 15, 143\u2013156 (2001).\n 37. Zhang, Y ., Li, C., Wang, P . & Li, B. Carbon-aware reinforcement learning for cloud computing. IEEE Trans. Sustain. Comput. 8, \n1\u201312 (2023).\n 38. Schwartz, R., Dodge, J., Smith, N. A. & Etzioni, O. Green ai. Commun. ACM 63, 54\u201363 (2020).\nAcknowledgements\nThe authors acknowledge the institutional support provided by Assiut University, including access to compu -\ntational resources and research facilities. No additional contributors beyond the authors meet the acknowledg-\nment criteria.\nAuthor contributions\nFady Nashat Manhary (FN) led the research, developed the proposed LSTM-MARL-Ape-X framework, and \nimplemented all core components, including workload forecasting, multi-agent learning, and distributed train-\ning. FN conducted the experiments, performed the full analysis, prepared all figures and tables, and wrote the \noriginal manuscript draft. Marghny H. Mohamed (MH) (CA) supervised the research process, reviewed the \nmethodology, and contributed feedback on the manuscript structure. MH also managed the submission and \ncorrespondence as the corresponding author. Mamdouh Farouk (MF) contributed to the interpretation of the \nresults and offered high-level feedback on the evaluation strategy. All authors reviewed and approved the final \nmanuscript.\nFunding\nOpen access funding provided by The Science, Technology & Innovation Funding Authority (STDF) in cooper-\nation with The Egyptian Knowledge Bank (EKB).\nDeclarations\nCompeting interests\nThe authors declare that they have no competing financial or non-financial interests relevant to the work \ndescribed in this manuscript.\nEthical approval\nThis study does not involve experiments on living vertebrates, higher invertebrates, or human subjects, and \ntherefore does not require ethical approval.\nConsent for publication\nThe results, data, and figures presented in this manuscript are original and have not been published previously. \nThis work is not under consideration for publication elsewhere.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.H.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article\u2019s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article\u2019s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\u00a9 The Author(s) 2025 \nScientific Reports |        (2025) 15:30567 17| https://doi.org/10.1038/s41598-025-14962-5\nwww.nature.com/scientificreports/", "mimetype": "text/plain", "start_char_idx": 1, "end_char_idx": 3696, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}